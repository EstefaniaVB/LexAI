{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d3c6989-a489-456d-a06f-8a437503fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta  # to add days or years\n",
    "from datetime import date\n",
    "import streamlit as st\n",
    "import requests\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from bokeh.models.widgets import Div\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a9e9f88-1c38-427f-9bb1-7ccc9719687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Technology'\n",
    "params = dict(q=query)\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "limit_date = today + relativedelta(days=-7)\n",
    "today_time = today.timestamp()\n",
    "limit_time = limit_date.timestamp()\n",
    "\n",
    "\n",
    "\n",
    "tweet_params = dict(q=query,\n",
    "                    filters=f\"timestamp < {limit_time}\",\n",
    "                    limit=100000)\n",
    "tweet_params_without_query = dict(q=\"\",\n",
    "                                  filters=f\"timestamp < {limit_time}\")\n",
    "\n",
    "headers = {'X-Meili-API-Key': 'OTkwNzQ0ZGRkZTc0NDcwM2RlMzFlOGIx'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_consultations():\n",
    "    lexai_url = \"http://127.0.0.1:7700/indexes/consultations/search\"\n",
    "    result = requests.get(lexai_url, params=params, headers=headers).json()\n",
    "    consultations = []\n",
    "    for i in result[\"hits\"]:\n",
    "        title = i['title']\n",
    "        topics = i['topics']\n",
    "        type_of_act = i['type_of_act']\n",
    "        try:\n",
    "            status = i[\"status\"]\n",
    "        except:\n",
    "            status= \"\"\n",
    "        try:\n",
    "            end_date = pd.to_datetime(i['end_date'],errors='coerce').date()\n",
    "        except:\n",
    "            end_date = pd.to_datetime(i['end_date'], errors='coerce')\n",
    "        link = i['link']\n",
    "        consultations.append({\n",
    "            \"title\": title,\n",
    "            \"status\": status,\n",
    "            \"topics\": topics,\n",
    "            \"type_of_act\": type_of_act,\n",
    "            \"end_date\": end_date,\n",
    "            \"link\": link\n",
    "        })\n",
    "\n",
    "    return consultations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d3af8fec-435a-4879-be34-72dcaeb62e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data from News\n",
    "lexai_url_news = \"http://127.0.0.1:7700/indexes/twitter_press/search\"\n",
    "news = requests.get(lexai_url_news, params=tweet_params,\n",
    "                    headers=headers).json()\n",
    "\n",
    "#Data from Politicians\n",
    "lexai_url_politicians = \"http://127.0.0.1:7700/indexes/twitter_politicians/search\"\n",
    "politicians = requests.get(lexai_url_politicians,\n",
    "                           params=tweet_params,\n",
    "                           headers=headers).json()\n",
    "\n",
    "#Data from General\n",
    "lexai_url_general = f\"http://127.0.0.1:7700/indexes/twitter_query/search/\"\n",
    "query_data_general = requests.get(lexai_url_general,\n",
    "                                  params=tweet_params,\n",
    "                                  headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ab0c128-2279-4d99-a145-c0b730b1b90c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentiment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-f98eb49801a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m values_query = [\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mall_tweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mall_tweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neutral'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mall_tweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'negative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentiment'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sentiment pie-charts\n",
    "#All data\n",
    "query_df = pd.DataFrame(query_data_general['hits'])\n",
    "politicians_df = pd.DataFrame(politicians['hits'])\n",
    "news_df = pd.DataFrame(news['hits'])\n",
    "all_tweets_df = pd.concat([query_df, politicians_df, news_df])\n",
    "\n",
    "colors_pie = ['#11E7FB', '#6082FD', '#731F7D']\n",
    "\n",
    "labels = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "\n",
    "values_query = [\n",
    "    all_tweets_df['sentiment'].value_counts()['positive'],\n",
    "    all_tweets_df['sentiment'].value_counts()['neutral'],\n",
    "    all_tweets_df['sentiment'].value_counts()['negative']\n",
    "]\n",
    "values_all = [41, 35, 24]\n",
    "\n",
    "# Create subplots: use 'domain' type for Pie subplot\n",
    "fig = make_subplots(rows=1,\n",
    "                    cols=2,\n",
    "                    specs=[[{\n",
    "                        'type': 'domain'\n",
    "                    }, {\n",
    "                        'type': 'domain'\n",
    "                    }]])\n",
    "fig.add_trace(\n",
    "    go.Pie(title='Twitter',\n",
    "           labels=labels,\n",
    "           values=values_all,\n",
    "           direction='clockwise',\n",
    "           sort=False), 1, 1)\n",
    "fig.add_trace(\n",
    "    go.Pie(title=query,\n",
    "        labels=labels,\n",
    "           values=values_query,\n",
    "           direction='clockwise',\n",
    "           sort=False), 1, 2)\n",
    "\n",
    "# Use `hole` to create a donut-like pie chart\n",
    "fig.update_traces(hole=.4,\n",
    "                  hoverinfo=\"label+percent\",\n",
    "                  textfont_size=15,\n",
    "                  marker=dict(colors=colors_pie))\n",
    "\n",
    "fig.update_layout(\n",
    "    # Add annotations in the center of the donut pies.\n",
    "    annotations=[\n",
    "        dict(text='',\n",
    "             x=0.18,\n",
    "             y=0.5,\n",
    "             font_size=18,\n",
    "             font_color='#731F7D',\n",
    "             showarrow=False),\n",
    "        dict(text=\"\",\n",
    "             x=0.82,\n",
    "             y=0.5,\n",
    "             font_size=18,\n",
    "             font_color='#731F7D',\n",
    "             showarrow=False)], autosize=False, width=500, height=250, margin=dict(l=0,r=0,b=0,t=0,pad=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "584572ec-df50-4942-8d3f-8be0b3f92135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_en</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>iso_lang</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>user_loc</th>\n",
       "      <th>user_desc</th>\n",
       "      <th>user_desc_en</th>\n",
       "      <th>user_image</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.622783e+09</td>\n",
       "      <td>1400711160509341698</td>\n",
       "      <td>hanisshit</td>\n",
       "      <td>tongue technology</td>\n",
       "      <td>tongue technology</td>\n",
       "      <td>2021/06/04 07:08:59</td>\n",
       "      <td>cs</td>\n",
       "      <td>cs</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>hani's shitpost acct</td>\n",
       "      <td>hani&amp;#39;s shitpost acct</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/7650178313...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/hanisshit/status/140071116...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.622783e+09</td>\n",
       "      <td>1400711066246545411</td>\n",
       "      <td>moothought</td>\n",
       "      <td>when is technology going to reach baby transla...</td>\n",
       "      <td>when is technology going to reach baby transla...</td>\n",
       "      <td>2021/06/04 07:08:36</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>438</td>\n",
       "      <td>Twitter-verse</td>\n",
       "      <td>( Insert Adjectives )</td>\n",
       "      <td>( Insert Adjectives )</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1375600698...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/moothought/status/14007110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.622783e+09</td>\n",
       "      <td>1400711150413754370</td>\n",
       "      <td>scrumnl</td>\n",
       "      <td>RT @rob_england: @technology @emilychangtv Say...</td>\n",
       "      <td>RT @rob_england: @technology @emilychangtv Say...</td>\n",
       "      <td>2021/06/04 07:08:56</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>469</td>\n",
       "      <td>Room 3B</td>\n",
       "      <td>Rincewindian Post-Agilist</td>\n",
       "      <td>Rincewindian Post-Agilist</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1227853908...</td>\n",
       "      <td></td>\n",
       "      <td>rob_england, technology, emilychangtv</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/scrumnl/status/14007111504...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.622783e+09</td>\n",
       "      <td>1400711119887646724</td>\n",
       "      <td>CantStopStompin</td>\n",
       "      <td>RT @veritasnewsfeed: mRNA technology pioneer s...</td>\n",
       "      <td>RT @veritasnewsfeed: mRNA technology pioneer s...</td>\n",
       "      <td>2021/06/04 07:08:49</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td></td>\n",
       "      <td>Never ever give up. 28. Gay. Trumplican.</td>\n",
       "      <td>Never ever give up. 28. Gay. Trumplican.</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1389693339...</td>\n",
       "      <td></td>\n",
       "      <td>veritasnewsfeed</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/CantStopStompin/status/140...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.622783e+09</td>\n",
       "      <td>1400711120067911683</td>\n",
       "      <td>hannahfrankbot</td>\n",
       "      <td>DOCTOR: Little bit of technology from my home.</td>\n",
       "      <td>DOCTOR: Little bit of technology from my home.</td>\n",
       "      <td>2021/06/04 07:08:49</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>http://abs.twimg.com/sticky/default_profile_im...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/hannahfrankbot/status/1400...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.622690e+09</td>\n",
       "      <td>1400318866757128192</td>\n",
       "      <td>euobs</td>\n",
       "      <td>The partnership will target technologies with ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021/06/03 05:10:09</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>197551</td>\n",
       "      <td>Brussels</td>\n",
       "      <td>Influential. Investigative. Independent. EUobs...</td>\n",
       "      <td>None</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1345057594...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://twitter.com/euobs/status/1400318866757...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.622641e+09</td>\n",
       "      <td>1400112472984457223</td>\n",
       "      <td>euobs</td>\n",
       "      <td>The EU is falling behind the US and China in #...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021/06/02 15:30:01</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>197551</td>\n",
       "      <td>Brussels</td>\n",
       "      <td>Influential. Investigative. Independent. EUobs...</td>\n",
       "      <td>None</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1345057594...</td>\n",
       "      <td>ArtificialIntelligence, blockchain</td>\n",
       "      <td>EIB</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://twitter.com/euobs/status/1400112472984...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.622721e+09</td>\n",
       "      <td>1400448440748945412</td>\n",
       "      <td>EURACTIV</td>\n",
       "      <td>.@EU_Commission announcement: Bill Gates-found...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021/06/03 13:45:02</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>137848</td>\n",
       "      <td>Brussels</td>\n",
       "      <td>EU news &amp; policy debates from the European Med...</td>\n",
       "      <td>None</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/8235903410...</td>\n",
       "      <td>Climate</td>\n",
       "      <td>EU_Commission, Breakthrough</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>https://twitter.com/EURACTIV/status/1400448440...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.622617e+09</td>\n",
       "      <td>1400014358260953089</td>\n",
       "      <td>EURACTIV</td>\n",
       "      <td>🇫🇮 What will drive Helsinki's transition to #C...</td>\n",
       "      <td>🇫🇮 What will drive Helsinki's transition to #C...</td>\n",
       "      <td>2021/06/02 09:00:08</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>137846</td>\n",
       "      <td>Brussels</td>\n",
       "      <td>EU news &amp; policy debates from the European Med...</td>\n",
       "      <td>EU news &amp; policy debates from the European Med...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/8235903410...</td>\n",
       "      <td>CarbonNeutrality</td>\n",
       "      <td>FredSimonEU</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>https://twitter.com/EURACTIV/status/1400014358...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.622798e+09</td>\n",
       "      <td>1400773332790829058</td>\n",
       "      <td>Ziare_com</td>\n",
       "      <td>Tehnologia 5 G, subiect amblu dezbatut</td>\n",
       "      <td>None</td>\n",
       "      <td>2021/06/04 11:16:02</td>\n",
       "      <td>ro</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>15751</td>\n",
       "      <td>Bucuresti, Romania</td>\n",
       "      <td>Stiri minut cu minut</td>\n",
       "      <td>None</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/5368434150...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/Ziare_com/status/140077333...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1163 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp                   id             user  \\\n",
       "0   1.622783e+09  1400711160509341698        hanisshit   \n",
       "1   1.622783e+09  1400711066246545411       moothought   \n",
       "2   1.622783e+09  1400711150413754370          scrumnl   \n",
       "3   1.622783e+09  1400711119887646724  CantStopStompin   \n",
       "4   1.622783e+09  1400711120067911683   hannahfrankbot   \n",
       "..           ...                  ...              ...   \n",
       "6   1.622690e+09  1400318866757128192            euobs   \n",
       "7   1.622641e+09  1400112472984457223            euobs   \n",
       "8   1.622721e+09  1400448440748945412         EURACTIV   \n",
       "9   1.622617e+09  1400014358260953089         EURACTIV   \n",
       "10  1.622798e+09  1400773332790829058        Ziare_com   \n",
       "\n",
       "                                                 text  \\\n",
       "0                                   tongue technology   \n",
       "1   when is technology going to reach baby transla...   \n",
       "2   RT @rob_england: @technology @emilychangtv Say...   \n",
       "3   RT @veritasnewsfeed: mRNA technology pioneer s...   \n",
       "4      DOCTOR: Little bit of technology from my home.   \n",
       "..                                                ...   \n",
       "6   The partnership will target technologies with ...   \n",
       "7   The EU is falling behind the US and China in #...   \n",
       "8   .@EU_Commission announcement: Bill Gates-found...   \n",
       "9   🇫🇮 What will drive Helsinki's transition to #C...   \n",
       "10            Tehnologia 5 G, subiect amblu dezbatut    \n",
       "\n",
       "                                              text_en                 date  \\\n",
       "0                                   tongue technology  2021/06/04 07:08:59   \n",
       "1   when is technology going to reach baby transla...  2021/06/04 07:08:36   \n",
       "2   RT @rob_england: @technology @emilychangtv Say...  2021/06/04 07:08:56   \n",
       "3   RT @veritasnewsfeed: mRNA technology pioneer s...  2021/06/04 07:08:49   \n",
       "4      DOCTOR: Little bit of technology from my home.  2021/06/04 07:08:49   \n",
       "..                                                ...                  ...   \n",
       "6                                                None  2021/06/03 05:10:09   \n",
       "7                                                None  2021/06/02 15:30:01   \n",
       "8                                                None  2021/06/03 13:45:02   \n",
       "9   🇫🇮 What will drive Helsinki's transition to #C...  2021/06/02 09:00:08   \n",
       "10                                               None  2021/06/04 11:16:02   \n",
       "\n",
       "   lang iso_lang  user_verified  followers_count            user_loc  \\\n",
       "0    cs       cs          False                8                       \n",
       "1    en       en          False              438       Twitter-verse   \n",
       "2    en       en          False              469             Room 3B   \n",
       "3    en       en          False               18                       \n",
       "4    en       en          False                0                       \n",
       "..  ...      ...            ...              ...                 ...   \n",
       "6    en     None           True           197551            Brussels   \n",
       "7    en     None           True           197551            Brussels   \n",
       "8    en     None           True           137848            Brussels   \n",
       "9    en     None           True           137846            Brussels   \n",
       "10   ro     None          False            15751  Bucuresti, Romania   \n",
       "\n",
       "                                            user_desc  \\\n",
       "0                                hani's shitpost acct   \n",
       "1                               ( Insert Adjectives )   \n",
       "2                           Rincewindian Post-Agilist   \n",
       "3            Never ever give up. 28. Gay. Trumplican.   \n",
       "4                                                       \n",
       "..                                                ...   \n",
       "6   Influential. Investigative. Independent. EUobs...   \n",
       "7   Influential. Investigative. Independent. EUobs...   \n",
       "8   EU news & policy debates from the European Med...   \n",
       "9   EU news & policy debates from the European Med...   \n",
       "10                               Stiri minut cu minut   \n",
       "\n",
       "                                         user_desc_en  \\\n",
       "0                            hani&#39;s shitpost acct   \n",
       "1                               ( Insert Adjectives )   \n",
       "2                           Rincewindian Post-Agilist   \n",
       "3            Never ever give up. 28. Gay. Trumplican.   \n",
       "4                                                None   \n",
       "..                                                ...   \n",
       "6                                                None   \n",
       "7                                                None   \n",
       "8                                                None   \n",
       "9   EU news & policy debates from the European Med...   \n",
       "10                                               None   \n",
       "\n",
       "                                           user_image  \\\n",
       "0   http://pbs.twimg.com/profile_images/7650178313...   \n",
       "1   http://pbs.twimg.com/profile_images/1375600698...   \n",
       "2   http://pbs.twimg.com/profile_images/1227853908...   \n",
       "3   http://pbs.twimg.com/profile_images/1389693339...   \n",
       "4   http://abs.twimg.com/sticky/default_profile_im...   \n",
       "..                                                ...   \n",
       "6   http://pbs.twimg.com/profile_images/1345057594...   \n",
       "7   http://pbs.twimg.com/profile_images/1345057594...   \n",
       "8   http://pbs.twimg.com/profile_images/8235903410...   \n",
       "9   http://pbs.twimg.com/profile_images/8235903410...   \n",
       "10  http://pbs.twimg.com/profile_images/5368434150...   \n",
       "\n",
       "                              hashtags                               mentions  \\\n",
       "0                                                                               \n",
       "1                                                                               \n",
       "2                                       rob_england, technology, emilychangtv   \n",
       "3                                                             veritasnewsfeed   \n",
       "4                                                                               \n",
       "..                                 ...                                    ...   \n",
       "6                                                                               \n",
       "7   ArtificialIntelligence, blockchain                                    EIB   \n",
       "8                              Climate            EU_Commission, Breakthrough   \n",
       "9                     CarbonNeutrality                            FredSimonEU   \n",
       "10                                                                              \n",
       "\n",
       "    retweet_count  favorite_count  \\\n",
       "0               0               0   \n",
       "1               0               0   \n",
       "2               2               0   \n",
       "3              57               0   \n",
       "4               0               0   \n",
       "..            ...             ...   \n",
       "6               0               1   \n",
       "7               0               1   \n",
       "8               0               6   \n",
       "9               4               3   \n",
       "10              0               0   \n",
       "\n",
       "                                                 link  \n",
       "0   https://twitter.com/hanisshit/status/140071116...  \n",
       "1   https://twitter.com/moothought/status/14007110...  \n",
       "2   https://twitter.com/scrumnl/status/14007111504...  \n",
       "3   https://twitter.com/CantStopStompin/status/140...  \n",
       "4   https://twitter.com/hannahfrankbot/status/1400...  \n",
       "..                                                ...  \n",
       "6   https://twitter.com/euobs/status/1400318866757...  \n",
       "7   https://twitter.com/euobs/status/1400112472984...  \n",
       "8   https://twitter.com/EURACTIV/status/1400448440...  \n",
       "9   https://twitter.com/EURACTIV/status/1400014358...  \n",
       "10  https://twitter.com/Ziare_com/status/140077333...  \n",
       "\n",
       "[1163 rows x 19 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4afb0503-24b6-4618-81a1-0ef332fef7b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-c0ac5c2375da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdotenv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'.env'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAnalyse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "from os.path import dirname, exists, join\n",
    "\n",
    "import gensim.downloader as api\n",
    "import meilisearch\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "load_dotenv(dotenv_path=join(dirname(dirname(__file__)),'.env'))\n",
    "\n",
    "class Analyse:\n",
    "    \n",
    "    def __init__(self, url='http://127.0.0.1:7700', key=None):\n",
    "        \n",
    "        if key is None:\n",
    "            key = os.getenv('MEILISEARCH_KEY')\n",
    "        \n",
    "        self.client = meilisearch.Client(url, key)\n",
    "        print\n",
    "        self.indices = [idx.get('name', None) for idx in self.client.get_indexes()]\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_words():\n",
    "        text_keys = {\n",
    "            'eurlex': ['title'],\n",
    "            'consultations': ['title', 'topics'],\n",
    "            'twitter_query': ['text_en', 'user_desc_en'],\n",
    "            'twitter_politicians': ['text_en', 'user_desc_en'],\n",
    "            'twitter_press': ['text_en', 'user_desc_en']\n",
    "        }\n",
    "        \n",
    "        print('Extracting all lemmatized words from json files... ', end='')\n",
    "        folder = join(os.path.abspath(''), 'data.json')\n",
    "        if not exists(folder):  # if folder doesn't exist\n",
    "            print('Folder not found.')\n",
    "            return\n",
    "\n",
    "        files = os.listdir(folder)\n",
    "        indices = [file.replace('.json', '') for file in files]\n",
    "        if len(files) == 0:  # if no files\n",
    "            print('No files found.')\n",
    "            return\n",
    "\n",
    "        words = []\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "        for index, filename in zip(indices, files):\n",
    "            with open(join(folder, filename), 'r') as file:\n",
    "                json_ = json.load(file)\n",
    "\n",
    "            for i in range(len(json_)):\n",
    "                for key in text_keys[index]:\n",
    "                    if json_[i][key] is not None:\n",
    "                        text = json_[i][key].lower().replace(r'\\d','')\n",
    "                        tokens = [lemmatizer.lemmatize(word) for word in tokenizer.tokenize(text) if word not in stop_words]\n",
    "                        words.extend(tokens)\n",
    "        \n",
    "        print('Done.\\n')\n",
    "        return words\n",
    "    \n",
    "    def get_synonyms(self, frac=0.03, n=0.5):\n",
    "        words = self.get_words()\n",
    "        \n",
    "        print('Loading model... ', end='')\n",
    "        word2vec_transfer = api.load('word2vec-google-news-300')\n",
    "        print('Done.')\n",
    "        \n",
    "        print('Getting available words in model... ', end='')\n",
    "        available_words = list(word2vec_transfer.key_to_index.keys())\n",
    "        print('Done.')\n",
    "        \n",
    "        print('Extracting nouns and adjectives...', end='')\n",
    "        words_to_keep = set([word for (word, tag) in nltk.pos_tag(words) if any(i in tag for i in ['NN', 'JJ'])])\n",
    "        print('Done.')\n",
    "        \n",
    "        print('Filtering words... ', end='')\n",
    "        word_counts = {k: v for k, v in Counter(words).items() if k in words_to_keep}\n",
    "        if isinstance(frac, int):\n",
    "            min_count = frac\n",
    "        else:\n",
    "            min_count = sorted(list(word_counts.values()))[::-1][int(len(word_counts.values()) * 0.03)]\n",
    "        words = [word for word in words_to_keep if (word_counts[word] > min_count and word in available_words)]\n",
    "        print('Done.')\n",
    "        \n",
    "        print('Finding synonyms... ', end='')\n",
    "        if n >= 1:\n",
    "            synonyms = {word: list(np.array(word2vec_transfer.most_similar(word, topn=n))[:,0]) for word in words}\n",
    "        else:\n",
    "            synonyms = {}\n",
    "            for word in words:\n",
    "                arr = np.array(word2vec_transfer.most_similar(word, topn=20))\n",
    "                synonyms[word] = list(arr[arr[:,1].astype(float) > n][:,0])\n",
    "        print('Done.')\n",
    "        \n",
    "        folder = join(os.path.abspath(''), 'data.json')\n",
    "        with open(join(folder, 'synonyms.json'), 'w') as file:\n",
    "            json.dump(synonyms, file)\n",
    "        \n",
    "        print('Saved synonyms to data.json/synonyms.json\\n')\n",
    "        return synonyms\n",
    "    \n",
    "    def get_all_sentiments(self):\n",
    "        folder = dirname(dirname(__file__))\n",
    "        if not exists(folder):  # if folder doesn't exist\n",
    "            print('Folder not found.')\n",
    "            return\n",
    "        \n",
    "        files = os.listdir(folder)\n",
    "        indices = [file.replace('.json', '') for file in files]\n",
    "        \n",
    "        if len(files) == 0:  # if no files\n",
    "            print('No files found.')\n",
    "            return\n",
    "        \n",
    "        for index, filename in zip(indices, files):\n",
    "            filepath = join(folder, 'data.json', filename)\n",
    "            with open(filepath, 'r') as file:\n",
    "                documents = json.load(file)\n",
    "            \n",
    "            filepath = join(folder, 'update.json', f'{index}_sentiment.json')\n",
    "            with open(filepath, 'w') as file:\n",
    "                json.dump(self.get_sentiments(documents), file)\n",
    "\n",
    "    \n",
    "    def get_sentiments(self, data):\n",
    "        #initialize sentiment analyzer\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "        # make dict of scores\n",
    "        sentiments = {i : {'id': doc['id'], \n",
    "                           'compound_score': sid.polarity_scores(doc['text_en'])['compound']}\n",
    "                      for i, doc in data.items()}\n",
    "\n",
    "        # add category\n",
    "        for k, v in sentiments.items():\n",
    "            if sentiments[k]['compound_score'] <= -0.2:\n",
    "                sentiments[k]['sentiment'] = 'negative'\n",
    "            elif sentiments[k]['compound_score'] > 0.2:\n",
    "                sentiments[k]['sentiment'] = 'positive'\n",
    "            else:\n",
    "                sentiments[k]['sentiment'] = 'neutral'\n",
    "            \n",
    "        return sentiments\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) > 2:\n",
    "        try:\n",
    "            kwargs = {kwarg.split(\"=\")[0]: eval(kwarg.split(\"=\")[1]) for kwarg in sys.argv[2:]}\n",
    "        except NameError:\n",
    "            kwargs = {kwarg.split(\"=\")[0]: kwarg.split(\"=\")[1].split(\",\") for kwarg in sys.argv[2:]}\n",
    "        getattr(Analyse(), sys.argv[1])(**kwargs)\n",
    "    elif len(sys.argv) == 2:\n",
    "        getattr(Analyse(), sys.argv[1])()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0c30b8b1-4bc2-43e6-946a-c6a67a913fa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__FILE__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-24c99ad857d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__FILE__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '__FILE__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.path.dirname(__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1a16b73-8d58-480c-914e-b5f4924cd26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(self, result, df):\n",
    "        entry = {}\n",
    "    \n",
    "        entry['id'] = result['id']\n",
    "        entry['user'] = result['user']['screen_name']\n",
    "        text = normalize('NFKD', result['full_text'])\n",
    "        entry['text'] = re.sub(r'https?:\\/\\/\\S*', '', text, flags=re.MULTILINE)\n",
    "        entry['text_en'] = self.gtrans(entry['text'], dest='en')\n",
    "        \n",
    "        dt = datetime.strptime(result['created_at'],\n",
    "                                \"%a %b %d %H:%M:%S %z %Y\")\n",
    "        entry['date'] = datetime.strftime(dt, \"%Y/%m/%d %H:%M:%S\")\n",
    "        entry['lang'] = result['lang']\n",
    "        entry['iso_lang'] = result.get('metadata', {}).get('iso_language_code', None)\n",
    "        \n",
    "        entry['user_verified'] = result['user']['verified']\n",
    "        entry['followers_count'] = result['user']['followers_count']\n",
    "        entry['user_loc'] =  normalize('NFKD', result['user']['location'])\n",
    "        entry['user_desc'] = normalize('NFKD', result['user']['description'])\n",
    "        entry['user_desc_en'] = self.gtrans(entry['user_desc'], dest='en')\n",
    "        entry['user_image'] = result['user']['profile_image_url']\n",
    "        \n",
    "        entry['hashtags'] = ', '.join([i['text'] \n",
    "                                        for i in result['entities']['hashtags']])\n",
    "        entry['mentions'] = ', '.join([i['screen_name'] \n",
    "                                        for i in result['entities']['user_mentions']])\n",
    "        entry['retweet_count'] = result['retweet_count']\n",
    "        entry['favorite_count'] = result['favorite_count']\n",
    "        \n",
    "        entry['timestamp'] = mktime(dt.timetuple())\n",
    "        entry['link'] = f\"https://twitter.com/{entry['user']}/status/{entry['id']}\"\n",
    "\n",
    "        user = entry['user'].lower().strip()\n",
    "        if all(col in df.columns for col in ['name', 'country', 'eu_group', 'national_group']):\n",
    "            entry['name'] = df[df['twitter'].str.lower() == user]['name'].iloc[0]\n",
    "            entry['country'] = df[df['twitter'].str.lower() == user]['country'].iloc[0]\n",
    "            entry['eu_group'] = df[df['twitter'].str.lower() == user]['eu_group'].iloc[0]\n",
    "            entry['national_group'] = df[df['twitter'].str.lower() == user]['national_group'].iloc[0]\n",
    "        elif 'media' in df.columns:\n",
    "            entry['name'] = df[df['twitter'].str.lower() == user]['media'].iloc[0]\n",
    "            entry['country'] = df[df['twitter'].str.lower() == user]['country'].iloc[0]\n",
    "        \n",
    "        if entry['text_en'] is not None:\n",
    "            sid = SentimentIntensityAnalyzer()\n",
    "            entry['compound_score'] = sid.polarity_scores(entry['text_en'])['compound']\n",
    "            if entry['compound_score'] <= -0.2:\n",
    "                entry['sentiment'] = 'negative'\n",
    "            elif entry['compound_score'] > 0.2:\n",
    "                entry['sentiment'] = 'positive'\n",
    "            else:\n",
    "                entry['sentiment'] = 'neutral'\n",
    "        \n",
    "        return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9eed5b3b-f450-40ac-afc4-6554a17984e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "meilisearch.index.Index"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(client.get_indexes()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7d0b543-fd08-4d8a-9834-4c439a07adf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-cee61ec0ae85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-67-cee61ec0ae85>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "indices = [idx.get('name', None) for idx in client.get_indexes()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b256b2-2564-4749-a23d-95b0050b3333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
